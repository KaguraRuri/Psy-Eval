# PsyEval: PsyEval: A Comprehensive Large Language Model Evaluation Benchmark for Mental Health

## Overview

PsyEval is a comprehensive benchmark designed to evaluate the performance of language models in the domain of mental health. This repository contains the necessary resources and documentation for understanding and replicating our experiments.


## Dataset

The dataset used in PsyEval is available in the [Datasets](datasets/). Please review the [data usage policy](processed/data-usage-policy.md) before using the dataset.

## Experiments

We conducted a series of experiments to evaluate various language models on mental health tasks. Detailed instructions for replicating these experiments can be found in the [Experiments](processed/experiments/) directory.

## Results

Our findings and results are summarized in the paper (link to the paper). For a detailed breakdown of the results, refer to the [Results](processed/results/) directory.