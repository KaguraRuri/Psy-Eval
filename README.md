# PsyEval: A Benchmark for Evaluating Language Models in Mental Health Tasks

## Overview

PsyEval(https://arxiv.org/abs/2311.09189) is a comprehensive benchmark designed to evaluate the performance of language models in the domain of mental health. This repository contains the necessary resources and documentation for understanding and replicating our experiments.


## Dataset

The dataset used in PsyEval is available in the [Datasets](datasets/). Please review the [data usage policy](processed/data-usage-policy.md) before using the dataset.

## Experiments

We conducted a series of experiments to evaluate various language models on mental health tasks. Detailed instructions for replicating these experiments can be found in the [Experiments](processed/experiments/) directory.

## Results

Our findings and results are summarized in the paper (link to the paper). For a detailed breakdown of the results, refer to the [Results](processed/results/) directory.


If you find the code and data useful, please cite our paper.

```bibtex
@article{jin2023psyeval,
  title={PsyEval: A Comprehensive Large Language Model Evaluation Benchmark for Mental Health},
  author={Jin, Haoan and Chen, Siyuan and Wu, Mengyue and Zhu, Kenny Q},
  journal={arXiv preprint arXiv:2311.09189},
  year={2023}
}